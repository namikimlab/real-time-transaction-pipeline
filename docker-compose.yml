x-airflow-common: &airflow-common
  build:
    context: ./airflow
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./dbt:/opt/airflow/dbt

services:

  kafka:
    image: bitnami/kafka:3.5
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_INTERNAL://:9094,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9094
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT_INTERNAL
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - ALLOW_PLAINTEXT_LISTENER=yes

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d

  metabase:
    image: metabase/metabase
    platform: linux/amd64
    ports:
      - "3000:3000"
    volumes:
      - metabase-data:/metabase-data
    environment:
      - MB_DB_FILE=/metabase-data/metabase.db

  spark-master:
    image: apache/spark:3.5.0
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master
    environment:
      - SPARK_MASTER_HOST=spark-master
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./spark/jars:/opt/spark/external-jars

  spark-worker:
    image: apache/spark:3.5.0
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8082:8080"
    volumes:
      - ./spark/jars:/opt/spark/external-jars

  spark-streaming:
    build:
      context: ./spark/streaming-app
    depends_on:
      - kafka
      - spark-master
      - spark-worker
      - postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    volumes:
      - ./spark/jars:/opt/spark/external-jars
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0
      --conf spark.jars.ivy=/tmp/.ivy2
      transaction_streaming.py

  airflow-init:
    <<: *airflow-common
    depends_on:
      - postgres
    command: db init

  airflow-webserver:
    <<: *airflow-common
    depends_on:
      - postgres
      - airflow-init
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      - postgres
      - airflow-init
    command: scheduler

volumes:
  pgdata:
  metabase-data:
